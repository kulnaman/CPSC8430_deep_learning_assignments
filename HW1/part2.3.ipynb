{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "from autograd_lib import autograd_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAD_NORM_MIN=1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(0.1, 1.0, num=10000)\n",
    "sin_function=lambda x:(np.sin(5*np.pi*x))/(5*(np.pi*x))\n",
    "y1=asarray([sin_function(e) for e in  x])\n",
    "def get_output():\n",
    "    return y1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(x,y,train_batchsize,test_batchsize):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 42)\n",
    "    X_train=torch.FloatTensor(X_train).unsqueeze(1).to(device)\n",
    "    X_test=torch.FloatTensor(X_test).unsqueeze(1).to(device)\n",
    "    y_train=torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "    y_test=torch.FloatTensor(y_test).unsqueeze(1).to(device)\n",
    "    train_data = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "    test_data = torch.utils.data.TensorDataset(X_test,y_test)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = train_batchsize, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = test_batchsize, shuffle = True)\n",
    "    return train_loader,test_loader,X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(1, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputDataLoader,model,loss_function,optimizer):\n",
    "    model.train()\n",
    "    min_loss=math.inf;\n",
    "    loss_array=[]\n",
    "    for (inputs,target) in inputDataLoader:\n",
    "        output=model.forward(inputs)\n",
    "        loss=loss_function(output,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "    return np.mean(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(inputDataLoader,model,loss_function,optimizer):\n",
    "    model.eval()\n",
    "    output_result=[]\n",
    "    loss_result=[]\n",
    "    with torch.no_grad():\n",
    "        for (inputs,target) in inputDataLoader:\n",
    "            inputs.to(device)\n",
    "            target.to(device)\n",
    "            output=model.forward(inputs)\n",
    "            loss=loss_function(output,target)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet()\n",
    "autograd_lib.register(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad_norm(model, criterion, train, target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    output = model(train)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = []\n",
    "    for p in model.modules():\n",
    "        if isinstance(p, nn.Linear):\n",
    "            param_norm = p.weight.grad.norm(2).item()\n",
    "            grads.append(param_norm)\n",
    "\n",
    "    grad_mean = np.mean(grads) # compute mean of gradient norms\n",
    "    return grad_mean,loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code from the official document https://github.com/cybertronai/autograd-lib\n",
    "\n",
    "# helper function to save activations\n",
    "def save_activations(layer, A, _):\n",
    "\n",
    "    activations[layer] = A\n",
    "\n",
    "# helper function to compute Hessian matrix\n",
    "def compute_hess(layer, _, B):\n",
    "\n",
    "    A = activations[layer]\n",
    "    BA = torch.einsum('nl,ni->nli', B, A) # do batch-wise outer product\n",
    "\n",
    "    # full Hessian\n",
    "    hess[layer] += torch.einsum('nli,nkj->likj', BA, BA) # do batch-wise outer product, then sum over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the minimum ratio\n",
    "def compute_minimum_ratio(model, criterion, train, target):\n",
    "    model.zero_grad()\n",
    "    # compute Hessian matrix\n",
    "    # save the gradient of each layer\n",
    "    with autograd_lib.module_hook(save_activations):\n",
    "        output = model(train)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "    # compute Hessian according to the gradient value stored in the previous step\n",
    "    with autograd_lib.module_hook(compute_hess):\n",
    "        autograd_lib.backward_hessian(output, loss='LeastSquares')\n",
    "\n",
    "    layer_hess = list(hess.values())\n",
    "    minimum_ratio = []\n",
    "\n",
    "    # compute eigenvalues of the Hessian matrix\n",
    "    for h in layer_hess:\n",
    "        size = h.shape[0] * h.shape[1]\n",
    "        h = h.reshape(size, size)\n",
    "        h_eig = torch.symeig(h).eigenvalues # torch.symeig() returns eigenvalues and eigenvectors of a real symmetric matrix\n",
    "        num_greater = torch.sum(h_eig > 0).item()\n",
    "        minimum_ratio.append(num_greater / len(h_eig))\n",
    "\n",
    "    ratio_mean = np.mean(minimum_ratio) # compute mean of minimum ratio\n",
    "\n",
    "    return ratio_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-2\n",
    "epochs=20\n",
    "loss_function=nn.MSELoss()\n",
    "batch_size=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,test_loader,x_train,y_train=createDataset(x,y1,batch_size,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrainModel():\n",
    "    model=get_model()\n",
    "    model.to(device)\n",
    "    optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    for epoch in range(0,epochs,1):\n",
    "        train(train_loader,model,loss_function,optimizer)\n",
    "        test(test_loader,model,loss_function,optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the main function to compute gradient norm and minimum ratio\n",
    "def main(model, train, target):\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    gradient_norm,loss = calculate_grad_norm(model, criterion, train, target)\n",
    "    minimum_ratio = compute_minimum_ratio(model, criterion, train, target)\n",
    "    print(loss.item())\n",
    "    print('gradient norm: {}, minimum ratio: {}'.format(gradient_norm, minimum_ratio))\n",
    "    return gradient_norm,loss,minimum_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.865282608079724e-05\n",
      "gradient norm: 0.04226916376501322, minimum ratio: 0.6174999999999999\n",
      "2.7523949029273354e-05\n",
      "gradient norm: 0.028945638448931277, minimum ratio: 0.605\n",
      "2.466772639309056e-05\n",
      "gradient norm: 0.023198812501505017, minimum ratio: 0.565\n",
      "0.00035169051261618733\n",
      "gradient norm: 0.10085293743759394, minimum ratio: 0.595\n",
      "2.8013108021696098e-05\n",
      "gradient norm: 0.010988960973918438, minimum ratio: 0.6125\n",
      "0.00010475878661964089\n",
      "gradient norm: 0.01689634658396244, minimum ratio: 0.5825\n",
      "1.665600939304568e-05\n",
      "gradient norm: 0.004436302348040044, minimum ratio: 0.5874999999999999\n",
      "0.00019340614380780607\n",
      "gradient norm: 0.06841663457453251, minimum ratio: 0.575\n",
      "0.00010785808262880892\n",
      "gradient norm: 0.05455744219943881, minimum ratio: 0.6174999999999999\n",
      "6.283984839683399e-05\n",
      "gradient norm: 0.04885400854982436, minimum ratio: 0.61\n",
      "0.00012083550245733932\n",
      "gradient norm: 0.0265196543186903, minimum ratio: 0.5625\n",
      "9.898041753331199e-05\n",
      "gradient norm: 0.060993243008852005, minimum ratio: 0.6025\n",
      "4.7693567466922104e-05\n",
      "gradient norm: 0.02197389886714518, minimum ratio: 0.6000000000000001\n",
      "8.531570347258821e-05\n",
      "gradient norm: 0.05457759276032448, minimum ratio: 0.59\n",
      "4.307496783440001e-05\n",
      "gradient norm: 0.012744762876536697, minimum ratio: 0.595\n",
      "4.5336568291531876e-05\n",
      "gradient norm: 0.03168123308569193, minimum ratio: 0.585\n",
      "0.0002250865800306201\n",
      "gradient norm: 0.09488074854016304, minimum ratio: 0.6125\n",
      "2.3705206331214868e-05\n",
      "gradient norm: 0.024811002193018794, minimum ratio: 0.5700000000000001\n",
      "7.535236363764852e-05\n",
      "gradient norm: 0.05656972946599126, minimum ratio: 0.5874999999999999\n",
      "0.0003639534115791321\n",
      "gradient norm: 0.1234358660876751, minimum ratio: 0.5825\n",
      "0.0001033762309816666\n",
      "gradient norm: 0.047844318207353354, minimum ratio: 0.5775\n",
      "2.569866228441242e-05\n",
      "gradient norm: 0.03201902494765818, minimum ratio: 0.58\n",
      "7.190112955868244e-05\n",
      "gradient norm: 0.049538425635546446, minimum ratio: 0.5925\n",
      "0.00016145166591741145\n",
      "gradient norm: 0.05714807286858559, minimum ratio: 0.5700000000000001\n",
      "6.218524504220113e-06\n",
      "gradient norm: 0.0026395811582915485, minimum ratio: 0.56\n",
      "1.1530106348800473e-05\n",
      "gradient norm: 0.002305071393493563, minimum ratio: 0.605\n",
      "0.00024116293934639543\n",
      "gradient norm: 0.0947451964020729, minimum ratio: 0.5675\n",
      "5.560707904805895e-06\n",
      "gradient norm: 0.006714364746585488, minimum ratio: 0.6125\n",
      "6.238371861400083e-05\n",
      "gradient norm: 0.04759810212999582, minimum ratio: 0.6\n",
      "0.0004090763977728784\n",
      "gradient norm: 0.10787790082395077, minimum ratio: 0.6174999999999999\n",
      "1.4944527720217593e-05\n",
      "gradient norm: 0.009177258238196373, minimum ratio: 0.6\n",
      "1.0574847692623734e-05\n",
      "gradient norm: 0.010913736652582884, minimum ratio: 0.6225\n",
      "0.00028445717180147767\n",
      "gradient norm: 0.06260000448673964, minimum ratio: 0.5925\n",
      "4.771620297105983e-06\n",
      "gradient norm: 0.007457863073796034, minimum ratio: 0.635\n",
      "5.3016763558844104e-05\n",
      "gradient norm: 0.04286662372760475, minimum ratio: 0.5825\n",
      "6.618440238526091e-06\n",
      "gradient norm: 0.012469433713704348, minimum ratio: 0.615\n",
      "0.00012279571092221886\n",
      "gradient norm: 0.0717259687371552, minimum ratio: 0.6\n",
      "0.00021184454089961946\n",
      "gradient norm: 0.0971641899086535, minimum ratio: 0.6025\n",
      "0.00022626483405474573\n",
      "gradient norm: 0.09337019082158804, minimum ratio: 0.5925\n",
      "2.6854200768866576e-05\n",
      "gradient norm: 0.023992613656446338, minimum ratio: 0.5925\n",
      "1.769825212249998e-05\n",
      "gradient norm: 0.015311088762246072, minimum ratio: 0.5700000000000001\n",
      "1.5577101294184104e-05\n",
      "gradient norm: 0.018620196962729096, minimum ratio: 0.59\n",
      "2.125283754139673e-05\n",
      "gradient norm: 0.013772719888947904, minimum ratio: 0.5974999999999999\n",
      "5.631684325635433e-05\n",
      "gradient norm: 0.04184848116710782, minimum ratio: 0.5875\n",
      "0.00012437351688276976\n",
      "gradient norm: 0.07579945772886276, minimum ratio: 0.5775\n",
      "8.788328705122694e-05\n",
      "gradient norm: 0.02945010457187891, minimum ratio: 0.61\n",
      "1.749512921378482e-05\n",
      "gradient norm: 0.020971354329958558, minimum ratio: 0.63\n",
      "0.0004077735065948218\n",
      "gradient norm: 0.1255484577268362, minimum ratio: 0.605\n",
      "3.368687612237409e-05\n",
      "gradient norm: 0.03329968941397965, minimum ratio: 0.595\n",
      "0.0002848465519491583\n",
      "gradient norm: 0.1065319050103426, minimum ratio: 0.5925\n",
      "3.1354600650956854e-05\n",
      "gradient norm: 0.029508044943213463, minimum ratio: 0.6174999999999999\n",
      "3.0280256396508776e-05\n",
      "gradient norm: 0.028888171771541238, minimum ratio: 0.6125\n",
      "1.9399391021579504e-05\n",
      "gradient norm: 0.018728360999375582, minimum ratio: 0.615\n",
      "1.6843032426550053e-05\n",
      "gradient norm: 0.019277975894510746, minimum ratio: 0.6174999999999999\n",
      "8.337294275406748e-05\n",
      "gradient norm: 0.008882290916517377, minimum ratio: 0.61\n",
      "4.6519908210029826e-06\n",
      "gradient norm: 0.0030106195772532374, minimum ratio: 0.6325000000000001\n",
      "5.5017968406900764e-05\n",
      "gradient norm: 0.03767092898488045, minimum ratio: 0.605\n",
      "6.75067349220626e-05\n",
      "gradient norm: 0.04612196097150445, minimum ratio: 0.605\n",
      "2.837224383256398e-05\n",
      "gradient norm: 0.03402802441269159, minimum ratio: 0.5874999999999999\n",
      "0.00035571554326452315\n",
      "gradient norm: 0.12206573411822319, minimum ratio: 0.59\n",
      "0.0001367087388643995\n",
      "gradient norm: 0.02554638567380607, minimum ratio: 0.5825\n",
      "0.00010519655188545585\n",
      "gradient norm: 0.029994031647220254, minimum ratio: 0.5675\n",
      "1.7017502614180557e-05\n",
      "gradient norm: 0.018658222863450646, minimum ratio: 0.61\n",
      "0.00020749715622514486\n",
      "gradient norm: 0.06308755837380886, minimum ratio: 0.615\n",
      "5.356721158022992e-05\n",
      "gradient norm: 0.04146714136004448, minimum ratio: 0.61\n",
      "1.2884194802609272e-05\n",
      "gradient norm: 0.017250626464374363, minimum ratio: 0.6225\n",
      "6.502356700366363e-05\n",
      "gradient norm: 0.03984376881271601, minimum ratio: 0.58\n",
      "0.0001409753895131871\n",
      "gradient norm: 0.03401532070711255, minimum ratio: 0.5925\n",
      "1.2858988156949636e-05\n",
      "gradient norm: 0.013958643889054656, minimum ratio: 0.61\n",
      "4.718784111901186e-06\n",
      "gradient norm: 0.008714065479580313, minimum ratio: 0.6275\n",
      "6.510112143587321e-05\n",
      "gradient norm: 0.04362966609187424, minimum ratio: 0.635\n",
      "0.00013974832836538553\n",
      "gradient norm: 0.04297859966754913, minimum ratio: 0.5900000000000001\n",
      "0.00010818583541549742\n",
      "gradient norm: 0.016892868909053504, minimum ratio: 0.6000000000000001\n",
      "0.0004593605117406696\n",
      "gradient norm: 0.12361270189285278, minimum ratio: 0.59\n",
      "0.000211667429539375\n",
      "gradient norm: 0.028848010813817382, minimum ratio: 0.5925\n",
      "0.0009339653770439327\n",
      "gradient norm: 0.18646426312625408, minimum ratio: 0.6074999999999999\n",
      "1.2391637028486002e-05\n",
      "gradient norm: 0.013339020079001784, minimum ratio: 0.6025\n",
      "0.0009405759046785533\n",
      "gradient norm: 0.2145058773458004, minimum ratio: 0.61\n",
      "2.7222642529523e-05\n",
      "gradient norm: 0.010619765962474048, minimum ratio: 0.575\n",
      "1.818528471630998e-05\n",
      "gradient norm: 0.02034222730435431, minimum ratio: 0.5925\n",
      "1.1816782716778107e-05\n",
      "gradient norm: 0.018596341367810965, minimum ratio: 0.59\n",
      "0.00027531024534255266\n",
      "gradient norm: 0.08257972495630383, minimum ratio: 0.575\n",
      "3.7159413750487147e-06\n",
      "gradient norm: 0.0017592019867151976, minimum ratio: 0.5874999999999999\n",
      "9.304187551606447e-05\n",
      "gradient norm: 0.06202313816174865, minimum ratio: 0.5925\n",
      "3.504180858726613e-05\n",
      "gradient norm: 0.034872140968218446, minimum ratio: 0.5874999999999999\n",
      "1.050429909810191e-05\n",
      "gradient norm: 0.0030791278113611042, minimum ratio: 0.6125\n",
      "8.310454177262727e-06\n",
      "gradient norm: 0.0019424328347668052, minimum ratio: 0.615\n",
      "2.743524055404123e-05\n",
      "gradient norm: 0.01620620535686612, minimum ratio: 0.6475\n",
      "0.0011690064566209912\n",
      "gradient norm: 0.2220663921907544, minimum ratio: 0.585\n",
      "0.00013324193423613906\n",
      "gradient norm: 0.07299130456522107, minimum ratio: 0.6\n",
      "0.00017717816808726639\n",
      "gradient norm: 0.08156040823087096, minimum ratio: 0.62\n",
      "1.2722944120469037e-05\n",
      "gradient norm: 0.015325086889788508, minimum ratio: 0.6074999999999999\n",
      "0.0004045340174343437\n",
      "gradient norm: 0.11387878935784101, minimum ratio: 0.59\n",
      "0.00019303335284348577\n",
      "gradient norm: 0.06897854385897517, minimum ratio: 0.5925\n",
      "0.00013772762031294405\n",
      "gradient norm: 0.0711305532604456, minimum ratio: 0.5900000000000001\n",
      "3.0391851396416314e-05\n",
      "gradient norm: 0.02529493172187358, minimum ratio: 0.605\n",
      "0.00010905722592724487\n",
      "gradient norm: 0.04697277117520571, minimum ratio: 0.5974999999999999\n",
      "0.00039558883872814476\n",
      "gradient norm: 0.12644518725574017, minimum ratio: 0.6425000000000001\n",
      "0.0002739549963735044\n",
      "gradient norm: 0.09250411577522755, minimum ratio: 0.5675\n",
      "1.4629804354626685e-05\n",
      "gradient norm: 0.019680686644278467, minimum ratio: 0.61\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(0)\n",
    "    # reset compute dictionaries\n",
    "\n",
    "    # compute Hessian\n",
    "    epochs=100\n",
    "    gradient_norm_array=[]\n",
    "    loss_array=[]\n",
    "    mi_array=[]\n",
    "    for i in range(0,epochs,1):\n",
    "        activations = defaultdict(int)\n",
    "        hess = defaultdict(float)\n",
    "        pretrain_model=pretrainModel()\n",
    "        autograd_lib.register(pretrain_model)\n",
    "\n",
    "        gradient_norm,loss,minimum_ratio=main(pretrain_model, x_train, y_train)\n",
    "        gradient_norm_array.append(gradient_norm)\n",
    "        loss_array.append(loss)\n",
    "        mi_array.append(minimum_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_array=[]\n",
    "final_mi_array=[]\n",
    "for i in range(0,len(gradient_norm_array),1):\n",
    "    if gradient_norm_array[i] < GRAD_NORM_MIN:\n",
    "        final_loss_array.append(loss_array[i].detach().cpu().numpy())\n",
    "        final_mi_array.append(mi_array[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"Minimum Ratio\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.scatter(final_mi_array,final_loss_array)\n",
    "plt.savefig(\"part2.3.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
