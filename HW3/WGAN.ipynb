{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import string\n",
    "from random import randint\n",
    "from collections import defaultdict, Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torchvision\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torchvision import utils\n",
    "from itertools import chain\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from inception_score import get_inception_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.conv1=nn.ConvTranspose2d(in_channels=100, out_channels=1024, kernel_size=4, stride=1, padding=0)\n",
    "#         self.batch1=nn.BatchNorm2d(num_features=1024)\n",
    "#         self.activ1=nn.ReLU()\n",
    "#         # State (1024x4x4)\n",
    "#         self.conv2=nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1)\n",
    "#         self.batch2=nn.BatchNorm2d(num_features=512)\n",
    "#         self.activ2=nn.ReLU()\n",
    "#         # State (512x8x8)\n",
    "#         self.conv3=nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1)\n",
    "#         self.batch3=nn.BatchNorm2d(num_features=256)\n",
    "#         self.activ3=nn.ReLU()\n",
    "#         # State (256x16x16)\n",
    "#         self.conv4=nn.ConvTranspose2d(in_channels=256, out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "#         self.output=nn.Tanh()\n",
    "#     def forward(self,x):\n",
    "#         x=self.conv1(x)\n",
    "#         x=self.batch1(x)\n",
    "#         x=self.activ1(x)\n",
    "#         x=self.conv2(x)\n",
    "#         x=self.batch2(x)\n",
    "#         x=self.activ2(x)     \n",
    "#         x=self.conv3(x)\n",
    "#         x=self.batch3(x)\n",
    "#         x=self.activ3(x)\n",
    "#         x=self.conv4(x)\n",
    "#         x=self.output(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(torch.nn.Module):\n",
    "#     def __init__(self, channels=3):\n",
    "#         super().__init__()\n",
    "#         # Filters [1024, 512, 256]\n",
    "#         # Input_dim = 100\n",
    "#         # Output_dim = C (number of channels)\n",
    "#         self.main_module = nn.Sequential(\n",
    "#             # Z latent vector 100\n",
    "#             nn.ConvTranspose2d(in_channels=100, out_channels=1024, kernel_size=4, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(num_features=1024),\n",
    "#             nn.ReLU(True),\n",
    "\n",
    "#             # State (1024x4x4)\n",
    "#             nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(num_features=512),\n",
    "#             nn.ReLU(True),\n",
    "\n",
    "#             # State (512x8x8)\n",
    "#             nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(num_features=256),\n",
    "#             nn.ReLU(True),\n",
    "\n",
    "#             # State (256x16x16)\n",
    "#             nn.ConvTranspose2d(in_channels=256, out_channels=channels, kernel_size=4, stride=2, padding=1))\n",
    "#             # output of main module --> Image (Cx32x32)\n",
    "\n",
    "#         self.output = nn.Tanh()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.main_module(x)\n",
    "#         return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(100, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.conv1=nn.Conv2d(in_channels=3, out_channels=256, kernel_size=4, stride=2, padding=1)\n",
    "#         self.activ1=nn.LeakyReLU(0.2, inplace=True)\n",
    "#         # State (1024x4x4)\n",
    "#         self.conv2=nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1)\n",
    "#         self.batch2=nn.BatchNorm2d(512)\n",
    "#         self.activ2=nn.LeakyReLU(0.2, inplace=True)\n",
    "#         self.conv3=nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1)\n",
    "#         self.batch3=nn.BatchNorm2d(1024)\n",
    "#         self.activ3=nn.LeakyReLU(0.2, inplace=True)\n",
    "#         # State (1024x4x4)\n",
    "\n",
    "#         self.conv4=nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=4, stride=1, padding=0)\n",
    "#     def forward(self,x):\n",
    "#         x=self.conv1(x)\n",
    "#         x=self.activ1(x)\n",
    "#         x=self.conv2(x)\n",
    "#         x=self.batch2(x)\n",
    "#         x=self.activ2(x)     \n",
    "#         x=self.conv3(x)\n",
    "#         x=self.batch3(x)\n",
    "#         x=self.activ3(x)\n",
    "#         x=self.conv4(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Discriminator(torch.nn.Module):\n",
    "#     def __init__(self, channels=3):\n",
    "#         super().__init__()\n",
    "#         # Filters [256, 512, 1024]\n",
    "#         # Input_dim = channels (Cx64x64)\n",
    "#         # Output_dim = 1\n",
    "#         self.main_module = nn.Sequential(\n",
    "#             # Image (Cx32x32)\n",
    "#             nn.Conv2d(in_channels=channels, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(num_features=256),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "#             # State (256x16x16)\n",
    "#             nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(num_features=512),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "#             # State (512x8x8)\n",
    "#             nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(num_features=1024),\n",
    "#             nn.LeakyReLU(0.2, inplace=True))\n",
    "#             # output of main module --> State (1024x4x4)\n",
    "\n",
    "#         self.output = nn.Sequential(\n",
    "#             # The output of D is no longer a probability, we do not apply sigmoid at the output of D.\n",
    "#             nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=4, stride=1, padding=0))\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.main_module(x)\n",
    "#         return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (3): BatchNorm1d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (6): BatchNorm1d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (9): BatchNorm1d(1024, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "    (12): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc_model=Discriminator()\n",
    "gen_model=Generator()\n",
    "disc_model.to(device)\n",
    "gen_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (3): BatchNorm1d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (6): BatchNorm1d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (9): BatchNorm1d(1024, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "    (12): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=512, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(batchsize):\n",
    "    trans = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='./files/', train=True, download=True, transform=trans)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='./files/', train=False, download=True, transform=trans)\n",
    "\n",
    "    \n",
    "    train_dataloader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = data_utils.DataLoader(test_dataset,  batch_size=batch_size, shuffle=True)\n",
    "    return train_dataloader,test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate=0.00005\n",
    "epochs=200\n",
    "batch_size=64\n",
    "critic_iter = 5\n",
    "generator_iters=15000\n",
    "weight_cliping_limit=0.01\n",
    "beta_1,beta_2=0.5,0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optim=torch.optim.RMSprop(gen_model.parameters(), lr=learn_rate)\n",
    "disc_optim=torch.optim.RMSprop(disc_model.parameters(), lr=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(gen,disc):\n",
    "    torch.save(gen.state_dict(), './generator_wgan.pkl')\n",
    "    torch.save(disc.state_dict(), './discriminator_wgan.pkl')\n",
    "    print('Models save to ./generator_wgan.pkl & ./discriminator_wgan.pkl ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_RATE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(train_loader):\n",
    "#     file = open(\"inception_score_graph_wgan.txt\", \"w\")\n",
    "\n",
    "#     # Now batches are callable self.data.next()\n",
    "#     data = collate_batches(train_loader)\n",
    "\n",
    "#     one = torch.FloatTensor(1, dtype=torch.float).to(device)\n",
    "#     mone = (one * -1).to(device)\n",
    "    \n",
    "#     for g_iter in tqdm(range(total_iterations)):\n",
    "\n",
    "#         # Requires grad, Generator requires_grad = False\n",
    "#         for para in disc_model.parameters():\n",
    "#             para.requires_grad = True\n",
    "\n",
    "#         # Train Dicriminator forward-loss-backward-update self.critic_iter times while 1 Generator forward-loss-backward-update\n",
    "#         for d_iter in range(critic_iter):\n",
    "#             disc_model.zero_grad()\n",
    "\n",
    "#             # Clamp parameters to a range [-c, c], c=self.weight_cliping_limit\n",
    "#             for para in disc_model.parameters():\n",
    "#                 para.data.clamp_(-weight_cliping_limit, weight_cliping_limit)\n",
    "\n",
    "#             images = data.__next__()\n",
    "#             # Check for batch to have full batch_size\n",
    "#             if (images.size()[0] != batch_size):\n",
    "#                 continue\n",
    "\n",
    "#             z = torch.rand((batch_size, 100, 1, 1)).to(device)\n",
    "#             images=images.to(device)\n",
    "\n",
    "\n",
    "#             # Train discriminator\n",
    "#             # WGAN - Training discriminator more iterations than generator\n",
    "#             # Train with real images\n",
    "#             d_loss_real = disc_model(images)\n",
    "#             d_loss_real = d_loss_real.mean(0).view(1)\n",
    "#             d_loss_real.backward(one)\n",
    "\n",
    "#             # Train with fake images\n",
    "#             z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
    "#             fake_images = gen_model(z)\n",
    "#             d_loss_fake = disc_model(fake_images)\n",
    "#             d_loss_fake = d_loss_fake.mean(0).view(1)\n",
    "#             d_loss_fake.backward(mone)\n",
    "\n",
    "#             d_loss = d_loss_fake - d_loss_real\n",
    "#             Wasserstein_D = d_loss_real - d_loss_fake\n",
    "#             disc_optim.step()\n",
    "# #             print(f'  Discriminator iteration: {d_iter}/{critic_iter}, loss_fake: {d_loss_fake.data}, loss_real: {d_loss_real.data}')\n",
    "\n",
    "\n",
    "\n",
    "#         # Generator update\n",
    "#         for para in disc_model.parameters():\n",
    "#             para.requires_grad = False  # to avoid computation\n",
    "\n",
    "#         gen_model.zero_grad()\n",
    "\n",
    "#         # Train generator\n",
    "#         # Compute loss with fake images\n",
    "#         z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
    "#         fake_images =  gen_model(z)\n",
    "#         g_loss = disc_model(fake_images)\n",
    "#         g_loss = g_loss.mean().mean(0).view(1)\n",
    "#         g_loss.backward(one)\n",
    "#         g_cost = -g_loss\n",
    "#         gen_optim.step()\n",
    "# #         print(f'Generator iteration: {g_iter}/{total_iterations}, g_loss: {g_loss.data}')\n",
    "#         if (g_iter) % SAVE_RATE == 0:\n",
    "#             # Workaround because graphic card memory can't store more than 830 examples in memory for generating image\n",
    "#             # Therefore doing loop and generating 800 examples and stacking into list of samples to get 8000 generated images\n",
    "#             # This way Inception score is more correct since there are different generated examples from every class of Inception model\n",
    "#             sample_list = []\n",
    "#             for i in range(10):\n",
    "#                 z = Variable(torch.randn(800, 100, 1, 1)).to(device)\n",
    "#                 samples = gen_model(z)\n",
    "#                 sample_list.append(samples.data.cpu().numpy())\n",
    "            \n",
    "#             # Flattening list of list into one list\n",
    "#             new_sample_list = list(chain.from_iterable(sample_list))\n",
    "# #             print(\"Calculating Inception Score over 8k generated images\")\n",
    "#             # Feeding list of numpy arrays\n",
    "#             inception_score = get_inception_score(new_sample_list, cuda=True, batch_size=32,\n",
    "#                                                   resize=True, splits=10)\n",
    "\n",
    "#             if not os.path.exists('training_result_images_wgan/'):\n",
    "#                 os.makedirs('training_result_images_wgan/')\n",
    "\n",
    "#             # Denormalize images and save them in grid 8x8\n",
    "#             z = Variable(torch.randn(800, 100, 1, 1)).to(device)\n",
    "#             samples = gen_model(z)\n",
    "#             samples = samples.mul(0.5).add(0.5)\n",
    "#             samples = samples.data.cpu()[:64]\n",
    "#             grid = utils.make_grid(samples)\n",
    "#             utils.save_image(grid, 'training_result_images_wgan/img_generatori_iter_{}_wgan.png'.format(str(g_iter).zfill(3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(train_loader):\n",
    "#         file = open(\"inception_score_graph_wgan.txt\", \"w\")\n",
    "\n",
    "#         # Now batches are callable self.data.next()\n",
    "#         data = collate_batches(train_loader)\n",
    "\n",
    "#         one = torch.FloatTensor([1]).to(device)\n",
    "#         mone = (one * -1).to(device)\n",
    "\n",
    "#         for g_iter in tqdm(range(generator_iters)):\n",
    "\n",
    "#             # Requires grad, Generator requires_grad = False\n",
    "#             for para in disc_model.parameters():\n",
    "#                 para.requires_grad = True\n",
    "\n",
    "#             # Train Dicriminator forward-loss-backward-update self.critic_iter times while 1 Generator forward-loss-backward-update\n",
    "#             for d_iter in range(critic_iter):\n",
    "#                 disc_model.zero_grad()\n",
    "\n",
    "#                 # Clamp parameters to a range [-c, c], c=self.weight_cliping_limit\n",
    "#                 for para in disc_model.parameters():\n",
    "#                     para.data.clamp_(-weight_cliping_limit, weight_cliping_limit)\n",
    "\n",
    "#                 images = data.__next__()\n",
    "#                 images=images.to(device)\n",
    "#                 # Check for batch to have full batch_size\n",
    "#                 if (images.size()[0] != batch_size):\n",
    "#                     continue\n",
    "\n",
    "#                 z = Variable(torch.randn((batch_size, 100, 1, 1))).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#                 # Train discriminator\n",
    "#                 # WGAN - Training discriminator more iterations than generator\n",
    "#                 # Train with real images\n",
    "#                 d_loss_real = disc_model(images)\n",
    "#                 d_loss_real = d_loss_real.mean(0).view(1)\n",
    "#                 d_loss_real.backward(one)\n",
    "\n",
    "#                 # Train with fake images\n",
    "#                 z = Variable(torch.randn((batch_size, 100, 1, 1))).to(device)\n",
    "#                 fake_images = gen_model(z)\n",
    "#                 d_loss_fake = disc_model(fake_images)\n",
    "#                 d_loss_fake = d_loss_fake.mean(0).view(1)\n",
    "#                 d_loss_fake.backward(mone)\n",
    "\n",
    "#                 d_loss = d_loss_fake - d_loss_real\n",
    "#                 Wasserstein_D = d_loss_real - d_loss_fake\n",
    "#                 disc_optim.step()\n",
    "# #                 print(f'  Discriminator iteration: {d_iter}/{critic_iter}, loss_fake: {d_loss_fake.data}, loss_real: {d_loss_real.data}')\n",
    "\n",
    "\n",
    "\n",
    "#             # Generator update\n",
    "#             for para in disc_model.parameters():\n",
    "#                 para.requires_grad = False  # to avoid computation\n",
    "\n",
    "#             gen_model.zero_grad()\n",
    "\n",
    "#             # Train generator\n",
    "#             # Compute loss with fake images\n",
    "#             z = Variable(torch.randn(batch_size, 100, 1, 1)).to(device)\n",
    "#             fake_images = gen_model(z)\n",
    "#             g_loss = disc_model(fake_images)\n",
    "#             g_loss = g_loss.mean().mean(0).view(1)\n",
    "#             g_loss.backward(one)\n",
    "#             g_cost = -g_loss\n",
    "#             gen_optim.step()\n",
    "# #             print(f'Generator iteration: {g_iter}/{self.generator_iters}, g_loss: {g_loss.data}')\n",
    "\n",
    "#             # Saving model and sampling images every 1000th generator iterations\n",
    "#             if (g_iter) % SAVE_RATE == 0:\n",
    "#                 # Workaround because graphic card memory can't store more than 830 examples in memory for generating image\n",
    "#                 # Therefore doing loop and generating 800 examples and stacking into list of samples to get 8000 generated images\n",
    "#                 # This way Inception score is more correct since there are different generated examples from every class of Inception model\n",
    "#                 sample_list = []\n",
    "#                 for i in range(10):\n",
    "#                     z = Variable(torch.randn(800, 100, 1, 1)).to(device)\n",
    "#                     samples = gen_model(z)\n",
    "#                     sample_list.append(samples.data.cpu().numpy())\n",
    "                \n",
    "#                 # Flattening list of list into one list\n",
    "#                 new_sample_list = list(chain.from_iterable(sample_list))\n",
    "# #                 print(\"Calculating Inception Score over 8k generated images\")\n",
    "#                 # Feeding list of numpy arrays\n",
    "#                 inception_score = get_inception_score(new_sample_list, cuda=True, batch_size=32,\n",
    "#                                                       resize=True, splits=10)\n",
    "\n",
    "#                 if not os.path.exists('training_result_images_wgan/'):\n",
    "#                     os.makedirs('training_result_images_wgan/')\n",
    "\n",
    "#                 # Denormalize images and save them in grid 8x8\n",
    "#                 z = Variable(torch.randn(800, 100, 1, 1)).to(device)\n",
    "#                 samples = gen_model(z)\n",
    "#                 samples = samples.mul(0.5).add(0.5)\n",
    "#                 samples = samples.data.cpu()[:64]\n",
    "#                 grid = utils.make_grid(samples)\n",
    "#                 utils.save_image(grid, 'training_result_images_wgan/img_generatori_iter_{}.png'.format(str(g_iter).zfill(3)))\n",
    "\n",
    "#                 # Testing\n",
    "                \n",
    "#         file.close()\n",
    "\n",
    "#         # Save the trained parameters\n",
    "#         save_model(gen_model,disc_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    file = open(\"inception_score_graph_wgan.txt\", \"w\")\n",
    "    generator_iter = 0\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        for i, (imgs, _) in enumerate(train_loader):\n",
    "\n",
    "            # Configure input\n",
    "            real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            disc_optim.zero_grad()\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], 100))))\n",
    "\n",
    "            # Generate a batch of images\n",
    "            fake_imgs = gen_model(z).detach()\n",
    "            # Adversarial loss\n",
    "            loss_disc = -torch.mean(disc_model(real_imgs)) + torch.mean(disc_model(fake_imgs))\n",
    "\n",
    "\n",
    "            loss_disc.backward()\n",
    "            disc_optim.step()\n",
    "\n",
    "            # Clip weights of discriminator\n",
    "            for p in disc_model.parameters():\n",
    "                p.data.clamp_(-weight_cliping_limit, weight_cliping_limit)\n",
    "\n",
    "            # Train the generator every n_critic iterations\n",
    "            if i % critic_iter == 0:\n",
    "\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                gen_optim.zero_grad()\n",
    "\n",
    "                # Generate a batch of images\n",
    "                gen_imgs = gen_model(z)\n",
    "                # Adversarial loss\n",
    "                loss_gen = -torch.mean(disc_model(gen_imgs))\n",
    "\n",
    "                loss_gen.backward()\n",
    "                gen_optim.step()\n",
    "\n",
    "\n",
    "            generator_iter+=1\n",
    "\n",
    "            if generator_iter % 1000 == 0:\n",
    "                # Workaround because graphic card memory can't store more than 830 examples in memory for generating image\n",
    "                # Therefore doing loop and generating 800 examples and stacking into list of samples to get 8000 generated images\n",
    "                # This way Inception score is more correct since there are different generated examples from every class of Inception model\n",
    "                sample_list = []\n",
    "                for i in range(10):\n",
    "                    z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], 100))))\n",
    "                    samples = gen_model(z)\n",
    "                    sample_list.append(samples.data.cpu().numpy())\n",
    "\n",
    "                # Flattening list of list into one list\n",
    "                new_sample_list = list(chain.from_iterable(sample_list))\n",
    "        #                 print(\"Calculating Inception Score over 8k generated images\")\n",
    "                # Feeding list of numpy arrays\n",
    "                inception_score = get_inception_score(new_sample_list, cuda=True, batch_size=32,\n",
    "                                                      resize=True, splits=10)\n",
    "\n",
    "                if not os.path.exists('training_result_images_wgan/'):\n",
    "                    os.makedirs('training_result_images_wgan/')\n",
    "\n",
    "                # Denormalize images and save them in grid 8x8\n",
    "                z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], 100))))\n",
    "                samples = gen_model(z)\n",
    "                samples = samples.mul(0.5).add(0.5)\n",
    "                samples = samples.data.cpu()[:64]\n",
    "                grid = utils.make_grid(samples)\n",
    "                utils.save_image(grid, 'training_result_images_wgan/img_generatori_iter_{}.png'.format(str(generator_iter).zfill(3)))\n",
    "                output = str(generator_iter) + \" \" + str(inception_score[0]) + \"\\n\"\n",
    "                file.write(output)\n",
    "                # Testing\n",
    "\n",
    "    file.close()\n",
    "\n",
    "            # Save the trained parameters\n",
    "    save_model(gen_model,disc_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batches( data_loader):\n",
    "        while True:\n",
    "            for i, (images, _) in enumerate(data_loader):\n",
    "                yield images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img(noise, number_of_images):\n",
    "    samples = gen(z).data.cpu().numpy()[:number_of_images]\n",
    "    generated_images = []\n",
    "    for sample in samples:\n",
    "        generated_images.append(sample.reshape(3, 32, 32))\n",
    "    return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(D_model_filename, G_model_filename):\n",
    "    D_model_path = os.path.join(os.getcwd(), D_model_filename)\n",
    "    G_model_path = os.path.join(os.getcwd(), G_model_filename)\n",
    "    disc_model.load_state_dict(torch.load(D_model_path))\n",
    "    gen_model.load_state_dict(torch.load(G_model_path))\n",
    "    print('Generator model loaded from {}.'.format(G_model_path))\n",
    "    print('Discriminator model loaded from {}-'.format(D_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:08<28:32,  8.60s/it]/home/nkulshr/.conda/envs/DL/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/home/nkulshr/DL_HW3/inception_score.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x).data.cpu().numpy()\n",
      "100%|██████████| 200/200 [31:02<00:00,  9.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models save to ./generator_wgan.pkl & ./discriminator_wgan.pkl \n",
      "Models save to ./generator_wgan.pkl & ./discriminator_wgan.pkl \n"
     ]
    }
   ],
   "source": [
    "train_loader,_=create_dataset(batch_size)\n",
    "train(train_loader)\n",
    "save_model(gen_model,disc_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader, D_model_path='./discriminator_dcgan.pkl', G_model_path='./generator_dcgan.pkl'):\n",
    "#     load_model( G_model_path,D_model_path)\n",
    "    z = Variable(Tensor(np.random.normal(0, 1, (batch_size, 100))))\n",
    "    samples = gen_model(z)\n",
    "    samples = samples.mul(0.5).add(0.5)\n",
    "    samples = samples.data.cpu()\n",
    "    grid = utils.make_grid(samples)\n",
    "    print(\"Grid of 8x8 images saved to 'wgan_model_image.png'.\")\n",
    "    utils.save_image(grid, 'wgan_model_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (6400x1 and 100x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m _,test_loader\u001b[38;5;241m=\u001b[39mcreate_dataset(batch_size)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(test_loader, D_model_path, G_model_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(test_loader, D_model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./discriminator_dcgan.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, G_model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./generator_dcgan.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     load_model( G_model_path,D_model_path)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43mgen_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      6\u001b[0m     samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m---> 22\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39mimg_shape)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (6400x1 and 100x128)"
     ]
    }
   ],
   "source": [
    "_,test_loader=create_dataset(batch_size)\n",
    "evaluate(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
